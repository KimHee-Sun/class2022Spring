{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimHee-Sun/class2022Spring/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltUO0sYwyGfU"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di6xZ08xsgO7"
      },
      "source": [
        "import nltk\n",
        "#natural language: 사람의 언어, artificial language 의 반대 (컴퓨터 language) -> text라고 생각하면 됨\n",
        "#package는 nltk, text processing을 다룰 것"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy a file from github\n",
        "import os\n",
        "url = \"https://raw.githubusercontent.com/hsnam95/class2022Spring/main/crime_punishment.txt\"\n",
        "os.system(\"curl \" + url + \" > crime_punishment.txt\")\n",
        "#방법 1: txt 파일을 url로 걸어서 실행하면 sample data가 생김\n",
        "# read a text file in the server\n",
        "file = open(\"crime_punishment.txt\")\n",
        "#open해서 file을 불러오고\n",
        "text = file.read().replace(\"\\n\", \" \")\n",
        "#모든 text를 string으로 가져오는 것 (중요)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "btgs9Nt-2Yj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbHEyyNHntcZ"
      },
      "source": [
        "# or copy/pase text here\n",
        "#방법 2\n",
        "text = 'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently — they’re not fond of rules. \\\n",
        "You can quote them, disagree with them, glorify or vilify them, \\\n",
        "but the only thing you can’t do is ignore them because they change things. \\\n",
        "They push the human race forward, and while some may see them as the crazy ones, we see genius, \\\n",
        "because the ones who are crazy enough to think that they can change the world, are the ones who do.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "- 긴 stirngs, 여러 문장으로 된 것을 하나하나 끊어서 들고 있는 것을 Token = 단어수준으로 끊는 것\n",
        "  - list 안에 단어들을 넣는 것이라고 생각하면 됨\n"
      ],
      "metadata": {
        "id": "DpPw_E9ICYix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"tmp.txt\", \"w\")\n",
        "#file을 output으로 write out할 수 있음 \n",
        "file.write(text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "9saTcc9C4Cjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45jV2UYs1GEC"
      },
      "source": [
        "text.split()\n",
        "#띄어쓰기를 기준으로 쪼개짐 -> list로 string을 쪼개 item으로 나누는 것 = 전형적인 Tokenization\n",
        "#one's도 하나로 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFR-cRaahTPy"
      },
      "source": [
        "' '.join(text.split())\n",
        "#다시 ' ' 띄어쓰기 단위로 join할 수도 있음 (' ' -> space를 string으로 만든 것)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq_lPZMHntcb"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "#punctuation에 대한 정의가 들어 있음\n",
        "words = word_tokenize(text)\n",
        "#위에 punctuation 들이 붙어 있는데, 이것조차 쪼개서 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "u-E5Of8rHuL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC1fe7nWF6wN"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "#Punctuation도 없애는 것, regular expression\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\")\n",
        "words = retokenize.tokenize(text)\n",
        "#regular expression 기반으로 자르는 것 (list로)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "Pt2XjPLpIUtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc42Plwx56YS"
      },
      "source": [
        "### Normalization  \n",
        "- cleaning, 깨끗하게 하는 것 -> 동사의 어미, 명사에 붙은 접두사 등을 분리하는 것등을 의미\n",
        "\n",
        "**Stemming** 어간 추출 대충의 패턴 규칙으로 어미를 잘라내는 것 (사전에 없는 어간 나올 수 있음)\n",
        "\n",
        "- 어간 추출: 대충 패턴 규칙으로 이 부분이 어미일 것이다~ 라고 잘라내는 것 (접두사, 접미사) -> 나름의 기계적인 규칙이라서 사전에 없는 것도 존재\n",
        "\n",
        "**Lemmatization** 표제어(기본 사전형) 추출\n",
        "\n",
        "- 표제어를 기반으로 변형시켜 원형으로 복귀\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsFfoAr259Fs"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "[stemmer.stem(w) for w in words]\n",
        "#list comprehension\n",
        "#stemmer!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkbgNiPd8BdL"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIh5pYd8f74"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPgIzrjm8_1N"
      },
      "source": [
        "### Stopword\n",
        "- 너무 많이 쓰는 관사, 대명사, 조동사 등은 관심이 없는 영역 -> contents가 없는 것들은 분석에 없으므로 따로 묶어놓기도 함\n",
        "- 한국어로 뭐라고 한다고,,?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdM2FaN8ntcc"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "nltk.download('stopwords')\n",
        "#contents가 없는 것들의 단어 list를 받아오는 것\n",
        "print(words)\n",
        "words = [w for w in words if not w in stopwords.words('english')]\n",
        "#list comprehension: words의 단어를 for loop로 받아서 검사를 하는 것 -> stop words에 들어있지 않으면 그것을 취하라 (stop words에 있는 것을 빼라)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmwXTL0UA5aw"
      },
      "source": [
        "### Collocation, Concordance\n",
        "- collocation: 연어, 'take care'\n",
        "- concordance: 특정 단어에 대해서 그 용례가 어떻게 되는 지 보여줄 수 있음 -> 조동사 would가 어디에 쓰였는 지 찾아줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fll4ygxNA3OJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef05f44e-c0de-4b8b-e40e-236b833c5875"
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "#저작권이 없는 것 text를 무료로 모아서 제공하는 것을 gutenberg\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "words = retokenize.tokenize(text)\n",
        "#단어별로 다 잘라서 list로 담는 것이 retokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0ZktoK3LRMS",
        "outputId": "67f1fadf-9c20-4ef7-a6c4-d3b725f8df91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161983"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVXlhIrAtmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d9802d-4241-49e5-f6f4-23cfdf68f44a"
      },
      "source": [
        "nltk.Text(words).collocations()  # default: (num=20, window_size=2)\n",
        "#20개의 collocation -> 연속해서 나오는 것\n",
        "#window size = 3을 해주면 단어가 세개가 연어로 된 것을 찾으라는 것임"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mrs Weston; Frank Churchill; Miss Woodhouse; Mrs Elton; Miss Bates;\n",
            "Jane Fairfax; Miss Fairfax; every thing; young man; every body; great\n",
            "deal; Mrs Goddard; dare say; Maple Grove; John Knightley; Miss Taylor;\n",
            "Miss Smith; Robert Martin; Colonel Campbell; Box Hill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq0wiutwA_au"
      },
      "source": [
        "nltk.Text(words).concordance('Emma', 79, 10)\n",
        "#emma라는 단어가 등장하는 앞 뒤로의 contexts를 같이 10개 뽑아라\n",
        "#79: text의 숫자가 79개"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIAIhXvP_BjU"
      },
      "source": [
        "nltk.Text(words).dispersion_plot([\"Emma\", \"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\"])\n",
        "#dispersion plot:16만개의 순서에서 emma라는 것이 등장하는 곳이 언제인 지"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWYZOFxq_ex2"
      },
      "source": [
        "# Distributional similarity: \n",
        "# find other words which appear in the same contexts as the specified word; \n",
        "# list most similar words first.\n",
        "nltk.Text(words).similar(\"Emma\")\n",
        "#분포상으로 유사한 게 무엇인가? emma와 양쪽 옆으로 나올 단어를 찾아봐라!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZihiVSBK_vy7"
      },
      "source": [
        "# Find contexts where the specified words appear; list most frequent common contexts first.\n",
        "nltk.Text(words).common_contexts([\"Emma\", \"she\"])\n",
        "#common contexts: specified 된 words 2개가 나타나는 contexts (공통으로 나타나는 양쪽 단어)를 말해라"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8TrCE14vGcT"
      },
      "source": [
        "### Frequency distribution, Frequency plot\n",
        "- text processing에서 가장 중요한 것이 Frequency: 어떤 것이 많이 쓰였는 지를 통해 document에 대한 정의를 내릴 수 있음 (특징을 잘 반영해주므로)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdY3m6zSBHic"
      },
      "source": [
        "fd = nltk.FreqDist(words).most_common(20)\n",
        "fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tpZThNV-ftv"
      },
      "source": [
        "nltk.Text(words).plot(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSOSzIovvKvE"
      },
      "source": [
        "### Dictionary\n",
        "- 사전, 통으로 불러와서 유용하게 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIcAOAvqntce"
      },
      "source": [
        "nltk.download('words')\n",
        "nltk.corpus.words.words('en')[-20:-1]\n",
        "#[-20:-1]은 제일 마지막 20개!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjAy_Ju7ntce"
      },
      "source": [
        "len(nltk.corpus.words.words('en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyIIqwosCRZa"
      },
      "source": [
        "### Extract information (pos tag, named entity)\n",
        "- pos tag는 품사, 정의가 설명되어 있음 (약어, 설명, 예) -> 자동으로 문장 내에서 품사를 분석해주는 것도 중요한 과정 = part of speech"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **POS tag list**:\n",
        "\n",
        "CC\tcoordinating conjunction \\\n",
        "CD\tcardinal digit \\\n",
        "DT\tdeterminer \\\n",
        "EX\texistential there (like: \"there is\" ... think of it like \"there exists\") \\\n",
        "FW\tforeign word \\\n",
        "IN\tpreposition/subordinating conjunction \\\n",
        "JJ\tadjective\t'big' \\\n",
        "JJR\tadjective, comparative\t'bigger' \\\n",
        "JJS\tadjective, superlative\t'biggest' \\\n",
        "LS\tlist marker\t1) \\\n",
        "MD\tmodal\tcould, will \\\n",
        "NN\tnoun, singular 'desk' \\\n",
        "NNS\tnoun plural\t'desks' \\\n",
        "NNP\tproper noun, singular\t'Harrison' \\\n",
        "NNPS\tproper noun, plural\t'Americans' \\\n",
        "PDT\tpredeterminer\t'all the kids' \\\n",
        "POS\tpossessive ending\tparent's \\\n",
        "PRP\tpersonal pronoun\tI, he, she \\\n",
        "PRP\\$\tpossessive pronoun\tmy, his, hers \\\n",
        "RB\tadverb\tvery, silently, \\\n",
        "RBR\tadverb, comparative\tbetter \\\n",
        "RBS\tadverb, superlative\tbest \\\n",
        "RP\tparticle\tgive up \\\n",
        "TO\tto\tgo 'to' the store. \\\n",
        "UH\tinterjection\terrrrrrrrm \\\n",
        "VB\tverb, base form\ttake \\\n",
        "VBD\tverb, past tense\ttook \\\n",
        "VBG\tverb, gerund/present participle\ttaking \\\n",
        "VBN\tverb, past participle\ttaken \\\n",
        "VBP\tverb, sing. present, non-3d\ttake \\\n",
        "VBZ\tverb, 3rd person sing. present\ttakes \\\n",
        "WDT\twh-determiner\twhich \\\n",
        "WP\twh-pronoun\twho, what \\\n",
        "WP\\$\tpossessive wh-pronoun\twhose \\\n",
        "WRB\twh-abverb\twhere, when \\"
      ],
      "metadata": {
        "id": "NLz0kp4kekaZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VBiObftCVwH"
      },
      "source": [
        "sent = \"I am Jhon from America and would like to go to Starbuck\"\n",
        "words = nltk.word_tokenize(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "H7h3wP_7OI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwKdu36WCewv"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = nltk.pos_tag(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos"
      ],
      "metadata": {
        "id": "Qg9VwV0HON1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnjGT1HpClE0"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "NE = nltk.ne_chunk(pos)\n",
        "# common Entity types: ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE"
      ],
      "metadata": {
        "id": "H_diaADpOWFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9DEIZ4lXQF"
      },
      "source": [
        "### Wordcloud\n",
        "- Frequency 기반 visualization -> text에 어떤 단어가 많이 나왔는 지! 단어 빈도별로 글자의 크기(font size)를 달리하여 visualize해줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jypxOnw9hoyZ"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "#string으로 다 담겨있음\n",
        "wc = WordCloud().generate(text) \n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6xv5ClAl5xk"
      },
      "source": [
        "stopwords = set(STOPWORDS) \n",
        "stopwords.add('unto')\n",
        "#unto를 빼고 싶다!\n",
        "wc = WordCloud(stopwords = stopwords).generate(text)\n",
        "#기존 stopwords에 unto를 더했음 -> 다른 stopwords 포함해서 unto까지 빠져있다는 걸 알 수 있음 \n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrVGVc0X9j7r"
      },
      "source": [
        "### Regular expression\n",
        "- 규칙을 표현하는 것: 어떠한 규칙을 표현해서 text에 조작을 가하고 싶을 때 규칙으로 해서 찾아냄 \n",
        "\n",
        "ex. words 파일에서 search를 할 수 있음\n",
        "- 여러 규칙들을 쓰는 것을 regular expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKgoQFI_cG-"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.search('a', 'abcdefa')\n",
        "#search: 주어진 string에 대해서 무언가를 찾는 것\n",
        "#오른쪽 항(string에 대해서) 왼쪽 항을 찾아라"
      ],
      "metadata": {
        "id": "qM5Uv5IyLeLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 앞부분만 찾아줌: 제일 처음 발견되는 a에 대해서만\n",
        "- span(0,1): 앞에 바로 있다 ex. 'ab'였다면 span=(0,2)"
      ],
      "metadata": {
        "id": "DlKmTKXaPgPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('a', 'abcdefa')\n",
        "#findall: 전부 찾아주는 것 -> string에서 찾아서 list로 만들어줌"
      ],
      "metadata": {
        "id": "KSVsBeO7LyOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub('a', 'b', 'abcdefa')\n",
        "#sub: replace, a를 b에 바꾸어라 (search처럼 처음에만 해당되는 게 아니라 전체에 해당)"
      ],
      "metadata": {
        "id": "BSCfOsKuLzGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **'a' 'b'처럼 직접적인 입력값을 줄 수도 있지만, 아예 규칙을 넣을 수도 있음**"
      ],
      "metadata": {
        "id": "E6bBGUDRSYMx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U2pS-NL9p38"
      },
      "source": [
        "'''       Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\n",
        "\n",
        ".\t        Wildcard, matches any character\n",
        "^abc\t    Matches some pattern abc at the start of a string\n",
        "abc$\t    Matches some pattern abc at the end of a string\n",
        "[abc]\t    Matches one of a set of characters\n",
        "[^abc]    Matches anything but a set of characters\n",
        "[A-Z0-9]\tMatches one of a range of characters\n",
        "ed|ing|s\tMatches one of the specified strings (disjunction)\n",
        "*\t        Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
        "+\t        One or more of previous item, e.g. a+, [a-z]+\n",
        "?\t        Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
        "{n}\t      Exactly n repeats where n is a non-negative integer\n",
        "{n,}\t    At least n repeats\n",
        "{,n}\t    No more than n repeats\n",
        "{m,n}\t    At least m and no more than n repeats\n",
        "a(b|c)+\t  Parentheses that indicate the scope of the operators\n",
        "(...)     Matches whatever regular expression is inside the parentheses\n",
        "\\d\n",
        "Matches any decimal digit; this is equivalent to the class [0-9].\n",
        "\\D\n",
        "Matches any non-digit character; this is equivalent to the class [^0-9].\n",
        "\\s\n",
        "Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v].\n",
        "\\S\n",
        "Matches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\f\\v].\n",
        "\\w\n",
        "Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
        "\\W\n",
        "Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp3_Dm9Q_tNQ"
      },
      "source": [
        "engdict = nltk.corpus.words.words('en')\n",
        "#list로 들어와 있는 것 <-> search와 findall은 'stirng'에 적용됨\n",
        "#engdict (0) = 'A'임 (stirng) -> 이 string에 대해서 search와 findall을 사용할 것임\n",
        "#regular expression이 string을 대상으로 한다는 걸 기억하기!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in engdict if re.search('ed$', w)]\n",
        "#engdict라는 list가 있는데, list comprehension을 이용해서 각각의 item을 w로 받아서 그 개수만큼 for loof를 도는 것\n",
        "#각 loof에 대해서 if가 적용 -> regular expresson search를 하는 것 -> 들어온 item, w에 대해서 ed$ (위에서는 'a'라고 썼는데 여기선 아예 regular expression)\n",
        "#읽는 방법: if에서 이 search가 w라는 string에 ed$가 있으면 w에 담기는 것\n",
        "#ed$:string의 끝(마지막점)에 'ed'가 있는 것을 찾아라/^abc: string의 앞에 (시작점)\n",
        "\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "oJR3c7-CTwUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.search('ed$','educated')"
      ],
      "metadata": {
        "id": "saf0_Q5vVpoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in engdict if re.search('^..j..t..$', w)]\n",
        "#string의 시작과 끝, .의 의미는 어떤 캐릭터가 와도 상관없다는 것이므로 (spelling spelling j spelling spelling t인 것을 찾아라)\n",
        "\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "AbEcdSVOWlUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in engdict if re.search('^[ghi][mno][jlk][def]$', w)]\n",
        "#대괄호:첫번째 캐릭터가 ghi 중에 하나, 그 다음은 mno 중에 하나~ -> 총 4개의 캐릭터! (기억하기)\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "Dx8fslShXVbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in engdict if re.search('^[ah]+$', w)][:10]\n",
        "#+: a혹은 h가 한번 이상 나오는 것을 찾아라\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "2Jn5CNWIXrsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1a5mQYj4hwn"
      },
      "source": [
        "nltk.download('treebank')\n",
        "wsj = nltk.corpus.treebank.words()\n",
        "\n",
        "result = [w for w in wsj if re.search('(ed|ing)$', w)]\n",
        "#(ed|ing): ed혹은 ing로 끝나는 단어를 찾아라\n",
        "\n",
        "result = sorted(set(result))\n",
        "#없으면 신문에 있는 순서대로 그냥 나오는 것, sort해주면 a to z 순서로\n",
        "print(result[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
        "#[0-9]는 숫자를 의미, \\=역슬래쉬, \\.을 쓰는 이유는 .이 any character가 아니라 그냥 .임을 의미하고 싶어서 앞에 \\를 붙여준 것\n",
        "#처음에 숫자가 하나 이상 나오고, 점이 나오고, 그 이후 숫자가 하나 이상 나오는 것을 찾아라 = 소수점이 들어간 수를 찾아라!\n",
        "\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "BYj6u_qwZW43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
        "#\\$는 실제 $를 search하고 싶어서! = 어떠한 대문자 알파벳이 하나 이상 나오고 달러표시로 끝나면 찾아라\n",
        "\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "7TDsugFKYu2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
        "#숫자가 4번 나오고 끝나는 것을 찾아라 (^로 시작해서 $로 끝나니까) = 네자리수\n",
        "\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "2bh09Mz1Zvwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
        "#숫자가 하나 이상 나오고, 대쉬가 나오고, 소문자가 3개에서 5개 사이인 것으로 끝나면 찾아라\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "3ZrXxjFYZyhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
        "#소문자가 5개 이상나오고, 대쉬가 나온 뒤, 소문자가 2개에서 3개 사이로 나오고 대쉬가 나온 뒤, 소문자가 6개 이하로 나오고 끝나는 것\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "2N72x-eJZ2wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "url = \"https://raw.githubusercontent.com/hsnam95/my/main/friends_season01_script.txt\"\n",
        "os.system(\"curl \" + url + \" > friends_season01_script.txt\")\n",
        "\n",
        "# read a text file in the server\n",
        "file = open(\"friends_season01_script.txt\")\n",
        "text = file.read()\n",
        "file.close()\n",
        "text\n",
        "#현재 string인 상태"
      ],
      "metadata": {
        "id": "05zHW52eKatf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = '(?<=: ).+(?=[\\.|\\?|\\!])'\n",
        "#(?<=내가 찾고자하는 것 앞에 나오는 것)(?=찾고자하는 것 뒤에 나오는 것)\n",
        "#(?<=내가 찾고자하는 것 앞에 나오는 것).(?=찾고자하는 것 뒤에 나오는 것) -> a로 시작하고 b로 끝나는 중간에 있는 하나의 캐릭터를 찾아라\n",
        "#앞에 조건, 나의 조건, 뒤의 조건 순서!\n",
        "#-> 왼쪽에 :이 있고 빈칸이 있고 캐릭터가 하나 이상 나오고, 마침표나 물음표, 느낌표로 끝나는 것을 찾아라\n",
        "sent = re.findall(pattern, text)]\n",
        "#findall은 규칙에 해당되는 것을 모두 찾아 list화해주는 것 (<->search)\n",
        "sent\n",
        "text = '\\n'.join(sent)\n",
        "#list로 만든 것을 결합하는 것, \\n은 space, 줄바꿈 -> 긴 줄로 바뀌는 것"
      ],
      "metadata": {
        "id": "8yOBHr-7Qw2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"tmp.txt\", \"w\")\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "CHZwCWeTUPgO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}